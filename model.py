# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1teaVJW05nGwO3HwAwLZhQAAJm0qMpK6B
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score
from fastcore.basics import *
from fastcore.parallel import *
from os import cpu_count

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/Malvertising/cicandmal2020-static.parquet')
df = df.drop(columns=['F0'])
df.shape

df[['F9500', 'F9501', 'F9502', 'F9503']] = df[['F9500', 'F9501', 'F9502', 'F9503']].fillna(0.0)
df.Label.value_counts()

df['Label'] = df['Label'].astype('object')
df.loc[df['Label'] != 'Benign', 'Label'] = 1
df.loc[df['Label'] == 'Benign', 'Label'] = 0
print(df['Label'].value_counts())
df['Label'] = df['Label'].astype(dtype=np.int32)

print(df.columns)

target = 'Label'
conts = list(df.columns.difference([target]).values)
len(conts)

df_train = df.sample(frac=0.8, replace=False)
df_test = df.drop(index=df_train.index)
df_train.shape, df_test.shape

def xs_y(df_, targ):
    if not isinstance(targ, list):
        xs = df_[df_.columns.difference([targ])].copy()
    else:
        xs = df_[df_.columns.difference(targ)].copy()
    y = df_[targ].copy()
    return xs, y

from sklearn.utils import resample
def undersample(df, target, n_samples):
    # Separate majority and minority classes
    df_majority = df[df[target] == 0]
    df_minority = df[df[target] == 1]

    # Check the size of each class
    n_majority = len(df_majority)
    n_minority = len(df_minority)

    # Calculate the number of samples to draw from each class to maintain the class proportion
    n_majority_sample = int(n_samples * (n_majority / (n_majority + n_minority)))
    n_minority_sample = n_samples - n_majority_sample

    # Resample the majority and minority classes
    df_majority_downsampled = resample(df_majority, replace=False, n_samples=n_majority_sample, random_state=42)
    df_minority_downsampled = resample(df_minority, replace=False, n_samples=n_minority_sample, random_state=42)

    # Combine the resampled majority and minority classes
    df_downsampled = pd.concat([df_majority_downsampled, df_minority_downsampled])

    return df_downsampled

# Undersample the training and testing datasets to 10,000 samples each
df_train_undersampled = undersample(df_train, target=target, n_samples=10000)
df_test_undersampled = undersample(df_test, target=target, n_samples=10000)

# Check the shape of the undersampled datasets
print(df_train_undersampled.shape, df_test_undersampled.shape)

X_train, y_train = xs_y(df_train_undersampled, targ=target)
X_test, y_test = xs_y(df_test_undersampled, targ=target)

# Train a Decision Tree classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions
y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)[:, 1]

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Print metrics
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print(f'ROC AUC: {roc_auc:.4f}')

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score
import numpy as np
import pandas as pd
from google.colab import drive
from sklearn.utils import resample


# Train a Random Forest classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions
y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)[:, 1]

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Print metrics
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print(f'ROC AUC: {roc_auc:.4f}')

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Assuming df_train and df_test are already defined

# Separate features and target
X_train, y_train = xs_y(df_train_undersampled, targ='Label')
X_test, y_test = xs_y(df_test_undersampled, targ='Label')

# Train XGBoost classifier
clf = XGBClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions
y_pred = clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')

from sklearn.metrics import classification_report
# Train and evaluate models
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42)
}

for model_name, model in models.items():
    print(f"Training {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    report = classification_report(y_test, y_pred)
    print(f"Classification Report for {model_name}:")
    print(report)
    print("="*60)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from sklearn.metrics import classification_report

# Reshape input data for CNN (assuming 1D Convolution)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Define CNN model
model = Sequential([
    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    MaxPooling1D(2),
    Conv1D(128, 3, activation='relu'),
    MaxPooling1D(2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the CNN model
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the CNN model
y_pred_proba = model.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype(np.int32)
report = classification_report(y_test, y_pred)
print("Classification Report for CNN Model:")
print(report)

# Print all column names
print(df.columns)

import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report

# Assuming X_train and X_test are 3D arrays
print(X_train.shape)  # Should print (n_samples, n_timesteps, n_features)

# Reshape the data to be 2D
n_samples, n_timesteps, n_features = X_train.shape
X_train_reshaped = X_train.reshape((n_samples, n_timesteps * n_features))
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1] * X_test.shape[2]))

# Define the Gradient Boosting model
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)

# Train the Gradient Boosting model
model.fit(X_train_reshaped, y_train)

# Evaluate the Gradient Boosting model
y_pred = model.predict(X_test_reshaped)
report = classification_report(y_test, y_pred)
print("Classification Report for Gradient Boosting Model:")
print(report)

from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report
import numpy as np
# Check the shape of your data
print(X_train.shape)  # Should print (n_samples, n_timesteps, n_features)

# Reshape the data to be 2D
n_samples, n_timesteps, n_features = X_train.shape
X_train_reshaped = X_train.reshape((n_samples, n_timesteps * n_features))
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1] * X_test.shape[2]))
# Define the AdaBoost model
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)

# Train the AdaBoost model
ada_model.fit(X_train_reshaped, y_train)

# Evaluate the AdaBoost model
ada_y_pred = ada_model.predict(X_test_reshaped)
ada_report = classification_report(y_test, ada_y_pred)
print("Classification Report for AdaBoost Model:")
print(ada_report)
# Define the XGBoost model
xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# Train the XGBoost model
xgb_model.fit(X_train_reshaped, y_train)

# Evaluate the XGBoost model
xgb_y_pred = xgb_model.predict(X_test_reshaped)
xgb_report = classification_report(y_test, xgb_y_pred)
print("Classification Report for XGBoost Model:")
print(xgb_report)

# def evaluate_one_feature(feature, index='', metric=roc_auc_score):
#     rootnode = DecisionTreeClassifier(max_depth=1, criterion='gini')
#     rootnode.fit(X_train[feature].array.reshape(-1,1), y_train)
#     preds = rootnode.predict(X_test[feature].array.reshape(-1,1))
#     preds_tr = rootnode.predict(X_train[feature].array.reshape(-1,1))
#     met = round(metric(y_test, preds), 4)
#     if met > 0.5:
#         return [feature, met, rootnode, preds, preds_tr]
#     else:
#         return [feature, met, None, [], []]

# evaluate_one_feature('F1')


# results = parallel(f=evaluate_one_feature,
#                   items=conts, n_workers=cpu_count(), threadpool=False, progress=True)

# result_df = pd.DataFrame(data=results, columns=['feature', 'roc_auc_score', 'fitted_models', 'predictions', 'preds_train']).sort_values(by='roc_auc_score', ascending=False)

# result_df[['feature', 'roc_auc_score']].head(15)

# import seaborn as sns
# import matplotlib.pyplot as plt

# fig, axes = plt.subplots(3,3, figsize=(12,12))
# axes = axes.flatten()
# for i, tf in enumerate(result_df['feature'].head(9)):
#     sns.histplot(data=df_train, x=tf, stat='percent', hue='Label', bins=100, log_scale=(False,True), ax=axes[i])

# useful_features = result_df.loc[result_df['roc_auc_score'] > 0.5]
# print(f"{len(useful_features)} / {len(conts)} features have direct separating power (linear)")

# ensemble_preds = np.mean(np.vstack(useful_features['predictions'].to_numpy()), axis=0)
# ensemble_preds.shape

# ensemble_preds_train = np.mean(np.vstack(useful_features['preds_train'].to_numpy()), axis=0)
# ensemble_preds_train.shape

# fpr, tpr, thresholds = roc_curve(y_train, ensemble_preds_train)
# # get the best threshold
# J = tpr - fpr
# ix = np.argmax(J)
# best_thresh = thresholds[ix]
# print("Best threshold", best_thresh)

# print("The Ensemble OneR model (simple average)")
# print("ROC-AUC", round(roc_auc_score(y_true=y_test, y_score=ensemble_preds),4))
# print("Precision", round(precision_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))
# print("Recall", round(recall_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))
# print("F1", round(f1_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))

# from sklearn.ensemble import RandomForestClassifier
# from sklearn.linear_model import LogisticRegression
# def evaluate_model(model, X_train, y_train, X_test, y_test, metric=roc_auc_score):
#     model.fit(X_train, y_train)
#     preds = model.predict(X_test)
#     preds_train = model.predict(X_train)
#     score = round(metric(y_test, preds), 4)
#     return score, preds, preds_train
# # Define the models
# models = {
#     'DecisionTree': DecisionTreeClassifier(max_depth=1, criterion='gini'),
#     'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
#     'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42)
# }

# # Store results
# model_results = []

# for model_name, model in models.items():
#     score, preds, preds_train = evaluate_model(model, X_train, y_train, X_test, y_test)
#     model_results.append({
#         'model': model_name,
#         'score': score,
#         'predictions': preds,
#         'predictions_train': preds_train
#     })
# model_results_df = pd.DataFrame(model_results).sort_values(by='score', ascending=False)
# print(model_results_df[['model', 'score']])

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score, classification_report
import numpy as np
import pandas as pd
# Assuming X_train and X_test are 3D arrays
print(X_train.shape)  # Should print (n_samples, n_timesteps, n_features)

# Reshape the data to be 2D
n_samples, n_timesteps, n_features = X_train.shape
X_train_reshaped = X_train.reshape((n_samples, n_timesteps * n_features))
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1] * X_test.shape[2]))
# Define models
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "AdaBoost": AdaBoostClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)
}

# Train and evaluate models
for model_name, model in models.items():
    print(f"Training {model_name}...")
    model.fit(X_train_reshaped, y_train)
    y_pred = model.predict(X_test_reshaped)
    y_pred_proba = model.predict_proba(X_test_reshaped)[:, 1] if hasattr(model, "predict_proba") else None

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba) if y_pred_proba is not None else (None, None, None)

    # Print classification report and additional metrics
    print(f"Classification Report for {model_name}:")
    print(classification_report(y_test, y_pred))
    print(f'Accuracy: {accuracy:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')
    if roc_auc is not None:
        print(f'ROC AUC: {roc_auc:.4f}')
    print("="*60)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from sklearn.metrics import classification_report

# Reshape input data for CNN (assuming 1D Convolution)
X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

# Define CNN model
model = Sequential([
    Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    MaxPooling1D(2),
    Conv1D(128, 3, activation='relu'),
    MaxPooling1D(2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the CNN model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the CNN model
y_pred_proba = model.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype(np.int32)
report = classification_report(y_test, y_pred)
print("Classification Report for CNN Model:")
print(report)

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve

# Sample data
models = ['Decision Tree', 'Random Forest', 'XGBoost', 'Gradient Boosting', 'AdaBoost', 'CNN']
accuracy = [0.9525, 0.9688, 0.9717, 0.9591, 0.9562, 0.979]
precision = [0.951, 0.9799, 0.978802, 0.9744, 0.971, 0.0]  # CNN Precision is missing, assuming placeholder
recall = [0.9626, 0.9633, 0.978, 0.9500, 0.9500, 0.0]  # CNN Recall is missing, assuming placeholder
f1_score = [0.9568, 0.9715, 0.9739, 0.9625, 0.9599, 0.0]  # CNN F1 Score is missing, assuming placeholder

# Bar chart for metric comparison
x = np.arange(len(models))
width = 0.2

fig, ax = plt.subplots(figsize=(10, 6))
bars1 = ax.bar(x - width*1.5, accuracy, width, label='Accuracy')
bars2 = ax.bar(x - width/2, precision, width, label='Precision')
bars3 = ax.bar(x + width/2, recall, width, label='Recall')
bars4 = ax.bar(x + width*1.5, f1_score, width, label='F1 Score')

ax.set_xlabel('Models')
ax.set_ylabel('Scores')
ax.set_title('Comparison of Model Performance Metrics')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

plt.show()

# Adding labels
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(bars1)
add_labels(bars2)
add_labels(bars3)
add_labels(bars4)

plt.show()

# Note: For ROC Curve, Precision-Recall Curve, and Confusion Matrix, you need the actual predictions and true labels.
# Example with random predictions and true labels for a single model (replace with actual predictions and labels):

# Dummy data for example (replace with your actual predictions and true labels)
y_true = np.random.randint(2, size=100)
y_scores = np.random.rand(100)

# ROC Curve
fpr, tpr, _ = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Precision-Recall Curve
precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_scores)

plt.figure()
plt.plot(recall_vals, precision_vals, color='b', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Confusion Matrix Heatmap
cm = confusion_matrix(y_true, y_scores.round())
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()